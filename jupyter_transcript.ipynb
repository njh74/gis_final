{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "os.listdir()\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sorry, that was a restaurant thing. I was going to just ask who did you receive the funds for? She's always just a possible for... She's just funding it in communications. So, lots of people reach out to her and she gets in touch with me. People reach out to her in some areas that are... They're seeing more of the other people, like needle debris or something like that. So she'll reach out to me and I'll go kind of suit the area and keep on it for a little bit. See how that's... how frequent. I wonder if it's like a citizen or if it's like businesses in the area. Because that's like, I guess, what would come to mind for me. Yeah, it's pretty even right now. Yeah, it's through the calls and there's control over it. How pretty is it? It seems like a good mixture now that it's warm out of here. Yeah, it's like if I could maybe... Yeah, you would know. Yeah, I guess you'd have to break it up. Well, Excel spreadsheet. Yeah, she does. How frequently do you get calls from this location? I think Excel is... I can't figure it out. This way. Is this having a chat? It would be obvious, like a big camp. A big camp. Well, a little camp. Yeah. Sorry. It's a really cute bird. Yeah, that's really cool. It's a big bird. Yeah. Oh, coffee. Dang. It looks like an asteroid. So slow down first, would you say? Of course. Yeah, I think so. Yeah, I think so. Yeah, yeah, I think so. Yeah, yeah, I think so. Let's see if we can get them to do that. So slow down first, would you say? Of course. Okay. Okay. Okay. Okay. Yeah. Okay. Yeah. That's the way to go. Yeah, so this is the situation I suppose if you ever out with encampment. What's this? I said if we're ever out with encampment, this is the point when you go. Yeah. Just turn off. Yeah.\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"medium.en\", device=\"cuda:0\")\n",
    "audio = whisper.load_audio('test2.mp3')\n",
    "\n",
    "result = model.transcribe(audio)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc802d56f714a12a967f9c447877ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097f0734e41f4d2da235363814a53764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\")\n",
    "# Explicitly set the max_seq_len\n",
    "config.max_seq_len = 4096\n",
    "config.max_answer_len= 4096\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. \n",
    "# Set to 0 if no GPU acceleration is available on your system.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    gpu_layers=50,\n",
    "    hf=True,\n",
    "    config=config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# # Set gpu_layers to the number of layers to offload to GPU. \n",
    "# # Set to 0 if no GPU acceleration is available on your system.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#                                              gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# Pipeline\n",
    "generator = HuggingFacePipeline(\n",
    "    pipeline=\n",
    "        pipeline(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            config=config,\n",
    "            task='text-generation',\n",
    "            max_new_tokens = 100,\n",
    "            device_map='auto'),\n",
    "    batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: 2\n"
     ]
    }
   ],
   "source": [
    "#Verifying LLM\n",
    "response = generator(\"What is 1+1?\")\n",
    "response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742\n",
      "too long\n",
      "511\n",
      "Sorry, that was a restaurant thing. I was going to just ask who did you receive the funds for? She's always just a possible for... She's just funding it in communications. So, lots of people reach out to her and she gets in touch with me. People reach out to her in some areas that are... They're seeing more of the other people, like needle debris or something like that. So she'll reach out to me and I'll go kind of suit the area and keep on it for a little bit. See how that's... how frequent. I wonder if i\n"
     ]
    }
   ],
   "source": [
    "#Truncating length\n",
    "print(len(result[\"text\"]))\n",
    "\n",
    "if len(result[\"text\"]) / 4 > 256:\n",
    "    print(\"too long\")\n",
    "    result[\"text\"] = result[\"text\"][1:512]\n",
    "else:\n",
    "    print(\"OK!\")\n",
    "\n",
    "print(len(result[\"text\"]))\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Restaurant\n",
      "* Funds\n",
      "* Communications\n",
      "* Needle debris\n",
      "* Area\n",
      "* Suit\n",
      "* Keep on it\n",
      "* Frequency\n",
      "* Wonder\n",
      "* I\n"
     ]
    }
   ],
   "source": [
    "#Prompting\n",
    "prompt = f\"\"\"\n",
    "Return any short phrases (4-5 words) that refer to a space, location or place from the following document in a bulleted list. Do not return anything else as part of your answer.\n",
    "Here is the following document:\n",
    "\\\"{result[\"text\"]}\\\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "response = generator(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
