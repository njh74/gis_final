{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "os.listdir()\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry, that was a restaurant thing. Oh, yeah, yeah, yeah. I was gonna just ask, like, who did you receive with? I'm sorry. I'm sorry, I'm sorry. Shon is funding education. Yeah. Okay, so lots of people reach out to her and then she gets a touch of me. They can flow reach out to her and stuff. The areas that are, they're seeing more like, people or like needles for you. Okay. What's that? She'll just, she'll reach out to me and then I'll go kind of to the area and keep on it for a little bit and see how it exists. How free is that? I was like, I wonder if it's like a citizen or if it's like a business area. That's like, I guess the two. Yeah. It's like coming to my community. Like, yes, it's like, pretty easy. Yeah, through the calls and how are we going to do this? How percity? It seems like a good mixture now that it's warm on us. Yeah, it's like, it's like, it's just really good. I don't know. It's really good. It's really good. Yeah, you and me. You just have it right now. Yeah, I guess we have to break it up. Well, I feel like she does this. Yeah, she does. How fine? Frequently, she's a good host from this location. I don't think I feel like I'm not off. I can't figure it out. I think that's right. Sorry. I'm just going to turn this off. I'm just going to turn this off. It's going to be obvious, like a big camp. I'm going to be here. Oh, it's going to be a camp. Oh, maybe it's going to be a camp. Yeah. Sounds like it. Sorry. Super. Yeah. That's really cool. Yeah, I know. I'm going to be here. I'm going to be here. I'm going to be here. Coffee. Okay. It looks like an acid. So let's go down first. Of course. I'm going to be here. Hello? Hello. Hello. Hello. Hello.\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base.en\")\n",
    "audio = whisper.load_audio('test2.mp3')\n",
    "\n",
    "result = model.transcribe(audio)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15f01e8113749b4ac8ee6bd64450ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7afb5c15ffe45aebe0ac7a7177b6768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\")\n",
    "# Explicitly set the max_seq_len\n",
    "config.max_seq_len = 4096\n",
    "config.max_answer_len= 4096\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. \n",
    "# Set to 0 if no GPU acceleration is available on your system.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    gpu_layers=100,\n",
    "    hf=True,\n",
    "    config=config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# # Set gpu_layers to the number of layers to offload to GPU. \n",
    "# # Set to 0 if no GPU acceleration is available on your system.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#                                              gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# Pipeline\n",
    "generator = HuggingFacePipeline(\n",
    "    pipeline=\n",
    "        pipeline(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            config=config,\n",
    "            task='text-generation',\n",
    "            max_new_tokens = 100,\n",
    "            device_map='auto'),\n",
    "    batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: 2\n"
     ]
    }
   ],
   "source": [
    "#Verifying LLM\n",
    "response = generator(\"What is 1+1?\")\n",
    "response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "OK!\n",
      "512\n",
      " I'm sorry, that was a restaurant thing. Oh, yeah, yeah, yeah. I was gonna just ask, like, who did you receive with? I'm sorry. I'm sorry, I'm sorry. Shon is funding education. Yeah. Okay, so lots of people reach out to her and then she gets a touch of me. They can flow reach out to her and stuff. The areas that are, they're seeing more like, people or like needles for you. Okay. What's that? She'll just, she'll reach out to me and then I'll go kind of to the area and keep on it for a little bit and see how\n"
     ]
    }
   ],
   "source": [
    "#Truncating length\n",
    "print(len(result[\"text\"]))\n",
    "\n",
    "if len(result[\"text\"]) / 4 > 256:\n",
    "    print(\"too long\")\n",
    "    result[\"text\"] = result[\"text\"][1:512]\n",
    "else:\n",
    "    print(\"OK!\")\n",
    "\n",
    "print(len(result[\"text\"]))\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Restaurant\n",
      "* Education\n",
      "* Shon\n",
      "* Touch\n",
      "* Flow\n",
      "* Area\n",
      "* Needles\n",
      "* Reach out\n",
      "* Go\n",
      "* Keep on it\n",
      "* Little bit\n",
      "* See how\n",
      "* Me\n",
      "* Kind of\n",
      "* Touch\n",
      "* Area\n",
      "* Needles\n",
      "* Reach out\n",
      "* Go\n",
      "* Keep on it\n",
      "* Little bit\n",
      "* See how\n",
      "* Me\n",
      "* Kind of\n",
      "* Touch\n",
      "* Area\n",
      "* Needles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prompting\n",
    "prompt = f\"\"\"\n",
    "Return any short phrases (4-5 words) that refer to a location or place from the following document in a bulleted list. Do not return anything else as part of your answer.\n",
    "Here is the following document:\n",
    "\\\"{result[\"text\"]}\\\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "response = generator(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
